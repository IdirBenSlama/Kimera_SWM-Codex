#!/usr/bin/env python3
"""
KIMERA SWM - Critical Fixes Validation Script
============================================

DO-178C Level A Testing Framework for Critical Component Fixes

This script validates the critical fixes applied to:
1. Zetetic Revolutionary Integration (get_api_settings import)
2. Universal Translator initialization methods
3. System component initialization integrity

Scientific Methodology:
- Zero-trust verification of each fix
- Empirical validation through actual instantiation
- Formal failure mode analysis
- Aerospace-grade reporting standards
"""

import sys
import os
import asyncio
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
import traceback

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

# Configure logging for aerospace-grade reporting
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('docs/reports/health/critical_fixes_validation.log')
    ]
)

logger = logging.getLogger(__name__)

class CriticalFixesValidator:
    """DO-178C Level A validator for critical component fixes."""

    def __init__(self):
        self.validation_results = {}
        self.test_start_time = datetime.now()
        self.critical_failures = []
        self.warnings = []

    async def validate_zetetic_revolutionary_integration(self) -> Dict[str, Any]:
        """Validate the zetetic revolutionary integration fix."""
        logger.info("üî¨ Testing Zetetic Revolutionary Integration fix...")

        try:
            # Test 1: Import validation
            from core.zetetic_and_revolutionary_integration.zetetic_revolutionary_integration_engine import (
                ZeteticRevolutionaryIntegrationEngine
            )
            logger.info("‚úÖ Zetetic engine import successful")

            # Test 2: Instantiation with get_api_settings
            start_time = time.time()
            engine = ZeteticRevolutionaryIntegrationEngine()
            init_time = time.time() - start_time

            logger.info(f"‚úÖ Zetetic engine instantiation successful ({init_time:.3f}s)")

            # Test 3: Settings access validation
            settings_available = hasattr(engine, 'settings') and engine.settings is not None
            logger.info(f"‚úÖ Settings validation: {'PASS' if settings_available else 'FAIL'}")

            return {
                "status": "SUCCESS",
                "import_test": True,
                "instantiation_test": True,
                "settings_test": settings_available,
                "init_time": init_time,
                "details": "All zetetic integration tests passed"
            }

        except Exception as e:
            logger.error(f"‚ùå Zetetic Revolutionary Integration validation failed: {e}")
            self.critical_failures.append(f"Zetetic Integration: {str(e)}")
            return {
                "status": "FAILED",
                "error": str(e),
                "traceback": traceback.format_exc()
            }

    async def validate_universal_translator_fix(self) -> Dict[str, Any]:
        """Validate the universal translator initialization fix."""
        logger.info("üî¨ Testing Universal Translator fix...")

        try:
            # Test 1: Import validation
            from engines.gyroscopic_universal_translator import GyroscopicUniversalTranslator
            logger.info("‚úÖ Universal translator import successful")

            # Test 2: Instantiation
            start_time = time.time()
            translator = GyroscopicUniversalTranslator()
            init_time = time.time() - start_time

            logger.info(f"‚úÖ Universal translator instantiation successful ({init_time:.3f}s)")

            # Test 3: Initialize cognitive systems method validation
            start_time = time.time()
            await translator.initialize_cognitive_systems()
            cognitive_init_time = time.time() - start_time

            logger.info(f"‚úÖ Cognitive systems initialization successful ({cognitive_init_time:.3f}s)")

            return {
                "status": "SUCCESS",
                "import_test": True,
                "instantiation_test": True,
                "cognitive_init_test": True,
                "init_time": init_time,
                "cognitive_init_time": cognitive_init_time,
                "details": "All universal translator tests passed"
            }

        except Exception as e:
            logger.error(f"‚ùå Universal Translator validation failed: {e}")
            self.critical_failures.append(f"Universal Translator: {str(e)}")
            return {
                "status": "FAILED",
                "error": str(e),
                "traceback": traceback.format_exc()
            }

    async def validate_cognitive_architecture_fix(self) -> Dict[str, Any]:
        """Validate the cognitive architecture core fix."""
        logger.info("üî¨ Testing Cognitive Architecture Core fix...")

        try:
            # Test 1: Import validation
            from core.cognitive_architecture_core import CognitiveArchitectureCore
            logger.info("‚úÖ Cognitive architecture import successful")

            # Test 2: Instantiation
            start_time = time.time()
            architecture = CognitiveArchitectureCore()
            init_time = time.time() - start_time

            logger.info(f"‚úÖ Cognitive architecture instantiation successful ({init_time:.3f}s)")

            # Test 3: Universal translator initialization method
            translator_init_method = hasattr(architecture, '_init_universal_translator')
            logger.info(f"‚úÖ Universal translator init method: {'AVAILABLE' if translator_init_method else 'MISSING'}")

            return {
                "status": "SUCCESS",
                "import_test": True,
                "instantiation_test": True,
                "translator_method_test": translator_init_method,
                "init_time": init_time,
                "details": "All cognitive architecture tests passed"
            }

        except Exception as e:
            logger.error(f"‚ùå Cognitive Architecture validation failed: {e}")
            self.critical_failures.append(f"Cognitive Architecture: {str(e)}")
            return {
                "status": "FAILED",
                "error": str(e),
                "traceback": traceback.format_exc()
            }

    async def validate_linguistic_intelligence_fix(self) -> Dict[str, Any]:
        """Validate the linguistic intelligence engine fix."""
        logger.info("üî¨ Testing Linguistic Intelligence Engine fix...")

        try:
            # Test 1: Import validation
            from engines.linguistic_intelligence_engine import LinguisticIntelligenceEngine
            logger.info("‚úÖ Linguistic intelligence import successful")

            # Test 2: Check initialization method
            engine = LinguisticIntelligenceEngine()
            translator_init_method = hasattr(engine, '_initialize_universal_translator')
            logger.info(f"‚úÖ Universal translator init method: {'AVAILABLE' if translator_init_method else 'MISSING'}")

            return {
                "status": "SUCCESS",
                "import_test": True,
                "instantiation_test": True,
                "translator_method_test": translator_init_method,
                "details": "All linguistic intelligence tests passed"
            }

        except Exception as e:
            logger.error(f"‚ùå Linguistic Intelligence validation failed: {e}")
            self.critical_failures.append(f"Linguistic Intelligence: {str(e)}")
            return {
                "status": "FAILED",
                "error": str(e),
                "traceback": traceback.format_exc()
            }

    async def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run comprehensive validation of all critical fixes."""
        logger.info("üöÄ Starting comprehensive critical fixes validation...")

        # Validate each critical fix
        validation_tasks = [
            ("zetetic_integration", self.validate_zetetic_revolutionary_integration()),
            ("universal_translator", self.validate_universal_translator_fix()),
            ("cognitive_architecture", self.validate_cognitive_architecture_fix()),
            ("linguistic_intelligence", self.validate_linguistic_intelligence_fix())
        ]

        for test_name, task in validation_tasks:
            try:
                result = await task
                self.validation_results[test_name] = result

                if result["status"] == "SUCCESS":
                    logger.info(f"‚úÖ {test_name}: PASSED")
                else:
                    logger.error(f"‚ùå {test_name}: FAILED")

            except Exception as e:
                logger.error(f"‚ùå {test_name}: EXCEPTION - {e}")
                self.validation_results[test_name] = {
                    "status": "EXCEPTION",
                    "error": str(e),
                    "traceback": traceback.format_exc()
                }

        # Generate comprehensive report
        return self.generate_validation_report()

    def generate_validation_report(self) -> Dict[str, Any]:
        """Generate comprehensive DO-178C Level A validation report."""
        total_tests = len(self.validation_results)
        passed_tests = sum(1 for result in self.validation_results.values()
                          if result.get("status") == "SUCCESS")

        test_duration = (datetime.now() - self.test_start_time).total_seconds()

        report = {
            "validation_summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "success_rate": (passed_tests / total_tests) * 100 if total_tests > 0 else 0,
                "test_duration": test_duration,
                "critical_failures": len(self.critical_failures),
                "warnings": len(self.warnings)
            },
            "detailed_results": self.validation_results,
            "critical_failures": self.critical_failures,
            "warnings": self.warnings,
            "compliance": {
                "do_178c_level": "A",
                "verification_method": "Empirical instantiation testing",
                "safety_impact": "Critical component initialization fixes",
                "next_actions": self._generate_next_actions()
            }
        }

        return report

    def _generate_next_actions(self) -> List[str]:
        """Generate next actions based on validation results."""
        actions = []

        if self.critical_failures:
            actions.append("Address critical component initialization failures")
            actions.append("Review import dependencies and path resolution")

        failed_tests = [name for name, result in self.validation_results.items()
                       if result.get("status") != "SUCCESS"]

        if failed_tests:
            actions.append(f"Fix failed component tests: {', '.join(failed_tests)}")

        if not actions:
            actions.append("Proceed with full system integration testing")
            actions.append("Monitor component performance during operational use")

        return actions

async def main():
    """Main validation execution."""
    try:
        # Create output directory
        os.makedirs("docs/reports/health", exist_ok=True)

        # Run validation
        validator = CriticalFixesValidator()
        report = await validator.run_comprehensive_validation()

        # Output results
        print("\n" + "="*80)
        print("üî¨ CRITICAL FIXES VALIDATION REPORT")
        print("="*80)

        summary = report["validation_summary"]
        print(f"üìä Test Summary:")
        print(f"   ‚Ä¢ Total Tests: {summary['total_tests']}")
        print(f"   ‚Ä¢ Passed: {summary['passed_tests']}")
        print(f"   ‚Ä¢ Failed: {summary['failed_tests']}")
        print(f"   ‚Ä¢ Success Rate: {summary['success_rate']:.1f}%")
        print(f"   ‚Ä¢ Duration: {summary['test_duration']:.2f}s")

        if summary['success_rate'] == 100.0:
            print("\n‚úÖ ALL CRITICAL FIXES VALIDATED SUCCESSFULLY")
            print("üöÄ System ready for continued integration")
        else:
            print(f"\n‚ö†Ô∏è {summary['failed_tests']} CRITICAL ISSUES REMAIN")
            print("üîß Manual intervention required")

        if report["critical_failures"]:
            print(f"\nüö® Critical Failures:")
            for failure in report["critical_failures"]:
                print(f"   ‚Ä¢ {failure}")

        print(f"\nüìÑ Detailed report saved to: docs/reports/health/critical_fixes_validation.log")
        print("="*80)

        return summary['success_rate'] == 100.0

    except Exception as e:
        logger.error(f"Validation execution failed: {e}")
        print(f"‚ùå Validation execution failed: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
