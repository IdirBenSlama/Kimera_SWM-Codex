"""
Kimera SWM - State-of-the-Art Monitoring Core
=============================================

Comprehensive monitoring system specifically designed for Kimera architecture.
Tracks absolutely everything in extreme detail using cutting-edge open-source tools.

Features:
- Real-time system monitoring
- Cognitive architecture metrics
- GPU/AI workload tracking
- Selective feedback analysis
- Revolutionary intelligence monitoring
- Security and compliance tracking
- Anomaly detection and alerting
"""

import asyncio
import time
import logging
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
import threading
from concurrent.futures import ThreadPoolExecutor

# Core monitoring infrastructure
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge, Summary, Info
from prometheus_fastapi_instrumentator import Instrumentator

# OpenTelemetry distributed tracing
# OpenTelemetry imports moved to optional section below

# Structured logging
import structlog
from loguru import logger

# System monitoring
import psutil
import GPUtil
try:
    import pynvml
    NVIDIA_MONITORING = True
except ImportError:
    NVIDIA_MONITORING = False

# Performance profiling
import tracemalloc
from memory_profiler import profile as memory_profile
# # import py_spy  # py-spy is a CLI tool, not a Python package  # py-spy is a CLI tool, not a Python package - REMOVED

# ML/AI monitoring
try:
    import wandb
    import mlflow
    MLOPS_MONITORING = True
except ImportError:
    MLOPS_MONITORING = False

# Anomaly detection
try:
    from pyod.models.iforest import IForest
    from adtk.detector import ThresholdAD, QuantileAD
    ANOMALY_DETECTION = True
except ImportError:
    ANOMALY_DETECTION = False

# Time series analysis
import numpy as np
import pandas as pd
from collections import deque, defaultdict

# Alerting
try:
    from slack_sdk import WebClient as SlackClient
    SLACK_ALERTS = True
except ImportError:
    SLACK_ALERTS = False

# OpenTelemetry imports - make optional
try:
    from opentelemetry import trace, metrics
    from opentelemetry.sdk.trace import TracerProvider
    from opentelemetry.sdk.trace.export import BatchSpanProcessor
    from opentelemetry.sdk.metrics import MeterProvider
    from opentelemetry.exporter.prometheus import PrometheusMetricReader
    from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
    from opentelemetry.instrumentation.requests import RequestsInstrumentor
    from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor
    OPENTELEMETRY_AVAILABLE = True
except ImportError:
    OPENTELEMETRY_AVAILABLE = False


class MonitoringLevel(Enum):
    """Monitoring detail levels"""
    MINIMAL = "minimal"
    STANDARD = "standard"
    DETAILED = "detailed"
    EXTREME = "extreme"


class AlertSeverity(Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class KimeraMetric:
    """Kimera-specific metric definition"""
    name: str
    description: str
    metric_type: str  # counter, gauge, histogram, summary
    labels: List[str] = field(default_factory=list)
    unit: str = ""
    namespace: str = "kimera"


@dataclass
class MonitoringAlert:
    """Monitoring alert definition"""
    id: str
    severity: AlertSeverity
    message: str
    timestamp: datetime
    metric_name: str
    value: float
    threshold: float
    context: Dict[str, Any] = field(default_factory=dict)


class KimeraMonitoringCore:
    """
    State-of-the-art monitoring system for Kimera SWM
    
    Provides comprehensive monitoring of:
    - System resources (CPU, memory, GPU, network, disk)
    - Kimera cognitive architecture (geoids, scars, contradictions)
    - AI/ML workloads (selective feedback, revolutionary intelligence)
    - Performance metrics (latency, throughput, errors)
    - Security and compliance (vulnerabilities, access patterns)
    - Custom Kimera-specific metrics
    """
    
    def __init__(self, 
                 monitoring_level: MonitoringLevel = MonitoringLevel.DETAILED,
                 enable_tracing: bool = True,
                 enable_profiling: bool = True,
                 enable_anomaly_detection: bool = True):
        
        self.monitoring_level = monitoring_level
        self.enable_tracing = enable_tracing
        self.enable_profiling = enable_profiling
        self.enable_anomaly_detection = enable_anomaly_detection
        
        # Monitoring state
        self.start_time = datetime.now()
        self.is_running = False
        self.alerts = deque(maxlen=10000)  # Keep last 10k alerts
        self.metrics_history = defaultdict(lambda: deque(maxlen=1000))
        
        # Threading
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.background_tasks = []
        
        # Initialize monitoring components
        self._init_prometheus_metrics()
        self._init_tracing()
        self._init_logging()
        self._init_anomaly_detection()
        self._init_alerting()
        
        # Kimera-specific monitoring
        self._init_kimera_metrics()
        
        logger.info("ðŸ” Kimera State-of-the-Art Monitoring Core initialized")
        logger.info(f"   ðŸ“Š Monitoring Level: {monitoring_level.value}")
        logger.info(f"   ðŸ”¬ Tracing: {'enabled' if enable_tracing else 'disabled'}")
        logger.info(f"   ðŸ“ˆ Profiling: {'enabled' if enable_profiling else 'disabled'}")
        logger.info(f"   ðŸš¨ Anomaly Detection: {'enabled' if enable_anomaly_detection else 'disabled'}")
    
    def _get_or_create_metric(self, metric_class, name, description, labels=None):
        """Get existing metric or create new one to avoid duplicates"""
        from prometheus_client import REGISTRY
        
        try:
            if labels:
                return metric_class(name, description, labels)
            else:
                return metric_class(name, description)
        except ValueError as e:
            # Metric already exists, find and return it
            for collector in REGISTRY._collector_to_names:
                if hasattr(collector, '_name') and collector._name == name:
                    logger.debug(f"Found existing metric: {name}")
                    return collector
            
            # If still not found, log warning and create with unique name
            logger.warning(f"Could not find existing metric {name}, creating with fallback")
            try:
                unique_name = f"{name}_{int(time.time())}"
                if labels:
                    return metric_class(unique_name, description, labels)
                else:
                    return metric_class(unique_name, description)
            except Exception as fallback_error:
                logger.error(f"Failed to create fallback metric for {name}: {fallback_error}")
                                 # Return a dummy metric that has the required methods but does nothing
                return self._create_dummy_metric()
    
    def _create_dummy_metric(self):
        """Create a dummy metric that doesn't break monitoring when real metrics fail"""
        class DummyMetric:
            def labels(self, **kwargs):
                return self
            def inc(self, value=1):
                pass
            def set(self, value):
                pass
            def observe(self, value):
                pass
        return DummyMetric()
    
    def _init_prometheus_metrics(self):
        """Initialize Prometheus metrics for Kimera"""
        
        # System metrics
        self.system_cpu_usage = self._get_or_create_metric(Gauge, 'kimera_system_cpu_percent', 'CPU usage percentage')
        self.system_memory_usage = self._get_or_create_metric(Gauge, 'kimera_system_memory_percent', 'Memory usage percentage') 
        self.system_disk_usage = self._get_or_create_metric(Gauge, 'kimera_system_disk_percent', 'Disk usage percentage')
        self.system_network_bytes = self._get_or_create_metric(Counter, 'kimera_system_network_bytes_total', 'Network bytes', ['direction'])
        
        # GPU metrics
        if NVIDIA_MONITORING:
            self.gpu_utilization = self._get_or_create_metric(Gauge, 'kimera_gpu_utilization_percent', 'GPU utilization', ['gpu_id'])
            self.gpu_memory_used = self._get_or_create_metric(Gauge, 'kimera_gpu_memory_used_bytes', 'GPU memory used', ['gpu_id'])
            self.gpu_temperature = self._get_or_create_metric(Gauge, 'kimera_gpu_temperature_celsius', 'GPU temperature', ['gpu_id'])
            self.gpu_power_draw = self._get_or_create_metric(Gauge, 'kimera_gpu_power_watts', 'GPU power draw', ['gpu_id'])
        
        # Application metrics
        self.request_duration = self._get_or_create_metric(Histogram, 'kimera_request_duration_seconds', 'Request duration', ['method', 'endpoint'])
        self.request_count = self._get_or_create_metric(Counter, 'kimera_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
        self.active_connections = self._get_or_create_metric(Gauge, 'kimera_active_connections', 'Active connections')
        self.error_count = self._get_or_create_metric(Counter, 'kimera_errors_total', 'Total errors', ['error_type', 'component'])
        
        # Kimera-specific metrics
        self.geoid_count = self._get_or_create_metric(Gauge, 'kimera_geoids_total', 'Total geoids')
        self.scar_count = self._get_or_create_metric(Gauge, 'kimera_scars_total', 'Total scars')
        self.contradiction_events = self._get_or_create_metric(Counter, 'kimera_contradictions_total', 'Contradiction events', ['type'])
        self.selective_feedback_operations = self._get_or_create_metric(Counter, 'kimera_selective_feedback_ops_total', 'Selective feedback operations', ['domain'])
        self.revolutionary_insights = self._get_or_create_metric(Counter, 'kimera_revolutionary_insights_total', 'Revolutionary insights generated')
        self.cognitive_cycles = self._get_or_create_metric(Counter, 'kimera_cognitive_cycles_total', 'Cognitive cycles completed')
        
        # Performance metrics
        self.embedding_latency = self._get_or_create_metric(Histogram, 'kimera_embedding_duration_seconds', 'Embedding generation latency')
        self.analysis_latency = self._get_or_create_metric(Histogram, 'kimera_analysis_duration_seconds', 'Analysis latency', ['type'])
        self.optimization_score = self._get_or_create_metric(Gauge, 'kimera_optimization_score', 'Current optimization score', ['domain'])
        self.consistency_score = self._get_or_create_metric(Gauge, 'kimera_consistency_score', 'Consistency score', ['component'])
        
        logger.info("ðŸ“Š Prometheus metrics initialized")
    
    def _init_tracing(self):
        """Initialize OpenTelemetry distributed tracing"""
        
        if not (self.enable_tracing and OPENTELEMETRY_AVAILABLE):
            logger.info("ðŸ”¬ Tracing disabled or OpenTelemetry not available")
            return
        
        try:
            # Set up tracer provider
            trace.set_tracer_provider(TracerProvider())
            tracer = trace.get_tracer(__name__)
            
            # Configure Jaeger exporter (disabled - deprecated)
            # jaeger_exporter = JaegerExporter(
            #     agent_host_name="localhost",
            #     agent_port=14268,
            # )
            # 
            # span_processor = BatchSpanProcessor(jaeger_exporter)
            # trace.get_tracer_provider().add_span_processor(span_processor)
            
            # Instrument frameworks
            FastAPIInstrumentor.instrument()
            RequestsInstrumentor.instrument()
            SQLAlchemyInstrumentor.instrument()
            
            self.tracer = tracer
            logger.info("ðŸ”¬ OpenTelemetry tracing initialized")
            
        except Exception as e:
            logger.warning(f"Failed to initialize tracing: {e}")
            self.enable_tracing = False
    
    def _init_logging(self):
        """Initialize structured logging"""
        
        # Configure structlog
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )
        
        # Configure loguru for additional structured logging
        logger.add(
            "logs/kimera_monitoring_{time:YYYY-MM-DD}.log",
            rotation="1 day",
            retention="30 days",
            level="INFO",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}",
            serialize=True
        )
        
        logger.info("ðŸ“ Structured logging initialized")
    
    def _init_anomaly_detection(self):
        """Initialize anomaly detection systems"""
        
        # Initialize basic anomaly detection state regardless
        self.anomaly_scores = defaultdict(list)
        self.anomaly_threshold = 0.5
        self.isolation_forest = None
        self.threshold_detectors = {}
        
        if not (self.enable_anomaly_detection and ANOMALY_DETECTION):
            logger.info("ðŸš¨ Anomaly detection disabled (missing libraries)")
            return
        
        try:
            # Isolation Forest for general anomaly detection
            self.isolation_forest = IForest(contamination=0.1, random_state=42)
            
            # Threshold-based detectors for specific metrics
            self.threshold_detectors = {
                'cpu_usage': ThresholdAD(high=90.0, low=0.0),
                'memory_usage': ThresholdAD(high=95.0, low=0.0),
                'error_rate': ThresholdAD(high=5.0, low=0.0),
                'response_time': QuantileAD(high=0.99, low=0.01)
            }
            
            logger.info("ðŸš¨ Anomaly detection systems initialized")
            
        except Exception as e:
            logger.warning(f"Failed to initialize anomaly detection: {e}")
            self.enable_anomaly_detection = False
            self.isolation_forest = None
            self.threshold_detectors = {}
    
    def _init_alerting(self):
        """Initialize alerting systems"""
        
        self.alert_channels = []
        
        # Slack alerting
        if SLACK_ALERTS:
            slack_token = os.getenv('SLACK_BOT_TOKEN')
            if slack_token:
                self.slack_client = SlackClient(token=slack_token)
                self.alert_channels.append('slack')
                logger.info("ðŸ“¢ Slack alerting initialized")
        
        # Email alerting (basic SMTP)
        smtp_host = os.getenv('SMTP_HOST')
        if smtp_host:
            self.smtp_config = {
                'host': smtp_host,
                'port': int(os.getenv('SMTP_PORT', 587)),
                'username': os.getenv('SMTP_USERNAME'),
                'password': os.getenv('SMTP_PASSWORD')
            }
            self.alert_channels.append('email')
            logger.info("ðŸ“§ Email alerting initialized")
        
        # Discord webhook alerting
        discord_webhook = os.getenv('DISCORD_WEBHOOK_URL')
        if discord_webhook:
            self.discord_webhook = discord_webhook
            self.alert_channels.append('discord')
            logger.info("ðŸŽ® Discord alerting initialized")
        
        if not self.alert_channels:
            logger.warning("âš ï¸ No alert channels configured")
    
    def _init_kimera_metrics(self):
        """Initialize Kimera-specific monitoring metrics"""
        
        # Define Kimera-specific metrics
        self.kimera_metrics = [
            KimeraMetric("geoid_creation_rate", "Rate of geoid creation", "counter", ["vault"]),
            KimeraMetric("scar_formation_rate", "Rate of scar formation", "counter", ["type"]),
            KimeraMetric("contradiction_severity", "Severity of contradictions", "histogram", ["source"]),
            KimeraMetric("selective_feedback_accuracy", "Accuracy of selective feedback", "gauge", ["domain"]),
            KimeraMetric("revolutionary_breakthrough_score", "Revolutionary breakthrough scores", "histogram"),
            KimeraMetric("cognitive_coherence", "Cognitive coherence metrics", "gauge", ["component"]),
            KimeraMetric("thermodynamic_entropy", "System thermodynamic entropy", "gauge"),
            KimeraMetric("vault_balance", "Vault balance metrics", "gauge", ["vault_id"]),
            KimeraMetric("embedding_similarity", "Embedding similarity scores", "histogram"),
            KimeraMetric("insight_generation_latency", "Insight generation latency", "histogram", ["type"]),
            KimeraMetric("constitutional_decisions", "Constitutional decisions by the Ethical Governor", "counter", ["verdict", "risk_category"]),
            KimeraMetric("ethics_processing_time", "Ethical decision processing time", "histogram"),
            KimeraMetric("constitutional_violations", "Constitutional violations detected", "counter", ["principle"]),
            KimeraMetric("conditional_approvals", "Conditional approvals granted", "counter", ["source_engine"]),
        ]
        
        # Create Prometheus metrics for each Kimera metric
        self.kimera_prometheus_metrics = {}
        for metric in self.kimera_metrics:
            metric_name = f"kimera_{metric.name}"
            if metric.metric_type == "counter":
                self.kimera_prometheus_metrics[metric.name] = self._get_or_create_metric(
                    Counter, metric_name, metric.description, metric.labels
                )
            elif metric.metric_type == "gauge":
                self.kimera_prometheus_metrics[metric.name] = self._get_or_create_metric(
                    Gauge, metric_name, metric.description, metric.labels
                )
            elif metric.metric_type == "histogram":
                self.kimera_prometheus_metrics[metric.name] = self._get_or_create_metric(
                    Histogram, metric_name, metric.description, metric.labels
                )
            elif metric.metric_type == "summary":
                self.kimera_prometheus_metrics[metric.name] = self._get_or_create_metric(
                    Summary, metric_name, metric.description, metric.labels
                )
        
        logger.info(f"ðŸ§  {len(self.kimera_metrics)} Kimera-specific metrics initialized")
    
    async def start_monitoring(self):
        """Start all monitoring systems"""
        
        if self.is_running:
            logger.warning("Monitoring already running")
            return
        
        self.is_running = True
        
        # Start system monitoring
        if self.monitoring_level in [MonitoringLevel.DETAILED, MonitoringLevel.EXTREME]:
            self.background_tasks.append(
                asyncio.create_task(self._monitor_system_resources())
            )
        
        # Start GPU monitoring
        if NVIDIA_MONITORING:
            self.background_tasks.append(
                asyncio.create_task(self._monitor_gpu_resources())
            )
        
        # Start application monitoring
        self.background_tasks.append(
            asyncio.create_task(self._monitor_application_metrics())
        )
        
        # Start Kimera-specific monitoring
        self.background_tasks.append(
            asyncio.create_task(self._monitor_kimera_systems())
        )
        
        # Start anomaly detection
        if self.enable_anomaly_detection:
            self.background_tasks.append(
                asyncio.create_task(self._monitor_anomalies())
            )
        
        # Start profiling if enabled
        if self.enable_profiling:
            self.background_tasks.append(
                asyncio.create_task(self._profile_performance())
            )
        
        logger.info("ðŸš€ Kimera monitoring systems started")
        logger.info(f"   ðŸ”„ Background tasks: {len(self.background_tasks)}")
    
    async def stop_monitoring(self):
        """Stop all monitoring systems"""
        
        if not self.is_running:
            return
        
        self.is_running = False
        
        # Cancel background tasks
        for task in self.background_tasks:
            task.cancel()
        
        # Wait for tasks to complete
        if self.background_tasks:
            await asyncio.gather(*self.background_tasks, return_exceptions=True)
        
        self.background_tasks.clear()
        
        # Shutdown executor
        self.executor.shutdown(wait=True)
        
        logger.info("ðŸ›‘ Kimera monitoring systems stopped")
    
    async def _monitor_system_resources(self):
        """Monitor system resources in real-time"""
        
        while self.is_running:
            try:
                # CPU metrics
                cpu_percent = psutil.cpu_percent(interval=1)
                self.system_cpu_usage.set(cpu_percent)
                self.metrics_history['cpu_usage'].append(cpu_percent)
                
                # Memory metrics
                memory = psutil.virtual_memory()
                self.system_memory_usage.set(memory.percent)
                self.metrics_history['memory_usage'].append(memory.percent)
                
                # Disk metrics
                disk = psutil.disk_usage('/')
                disk_percent = (disk.used / disk.total) * 100
                self.system_disk_usage.set(disk_percent)
                
                # Network metrics
                network = psutil.net_io_counters()
                if self.system_network_bytes:
                    self.system_network_bytes.labels(direction='sent').inc(network.bytes_sent)
                    self.system_network_bytes.labels(direction='recv').inc(network.bytes_recv)
                
                # Check for alerts
                await self._check_system_alerts(cpu_percent, memory.percent, disk_percent)
                
                await asyncio.sleep(5)  # Update every 5 seconds
                
            except Exception as e:
                logger.error(f"Error monitoring system resources: {e}")
                await asyncio.sleep(10)
    
    async def _monitor_gpu_resources(self):
        """Monitor GPU resources using NVIDIA ML"""
        
        if not NVIDIA_MONITORING:
            return
        
        try:
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            
            while self.is_running:
                try:
                    for i in range(device_count):
                        handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                        
                        # GPU utilization
                        utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                        self.gpu_utilization.labels(gpu_id=str(i)).set(utilization.gpu)
                        
                        # Memory usage
                        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                        self.gpu_memory_used.labels(gpu_id=str(i)).set(memory_info.used)
                        
                        # Temperature
                        temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                        self.gpu_temperature.labels(gpu_id=str(i)).set(temp)
                        
                        # Power draw
                        try:
                            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert to watts
                            self.gpu_power_draw.labels(gpu_id=str(i)).set(power)
                        except pynvml.NVMLError:
                            pass  # Power monitoring not supported on all GPUs
                    
                    await asyncio.sleep(5)
                    
                except Exception as e:
                    logger.error(f"Error monitoring GPU resources: {e}")
                    await asyncio.sleep(10)
                    
        except Exception as e:
            logger.error(f"Failed to initialize GPU monitoring: {e}")
    
    async def _monitor_application_metrics(self):
        """Monitor application-level metrics"""
        
        while self.is_running:
            try:
                # Track active connections (example)
                # This would be integrated with FastAPI middleware
                
                # Error rate calculation
                error_rate = self._calculate_error_rate()
                self.metrics_history['error_rate'].append(error_rate)
                
                await asyncio.sleep(10)
                
            except Exception as e:
                logger.error(f"Error monitoring application metrics: {e}")
                await asyncio.sleep(15)
    
    async def _monitor_kimera_systems(self):
        """Monitor Kimera-specific systems and components"""
        
        while self.is_running:
            try:
                # This would integrate with actual Kimera components
                # For now, we'll create placeholder monitoring
                
                # Monitor geoid count (would connect to vault manager)
                # geoid_count = await self._get_geoid_count()
                # self.geoid_count.set(geoid_count)
                
                # Monitor scar count
                # scar_count = await self._get_scar_count()
                # self.scar_count.set(scar_count)
                
                # Monitor cognitive coherence
                # coherence = await self._calculate_cognitive_coherence()
                # self.kimera_prometheus_metrics['cognitive_coherence'].labels(component='overall').set(coherence)
                
                await asyncio.sleep(15)
                
            except Exception as e:
                logger.error(f"Error monitoring Kimera systems: {e}")
                await asyncio.sleep(20)
    
    async def _monitor_anomalies(self):
        """Monitor for anomalies using machine learning"""
        
        if not self.enable_anomaly_detection or not self.isolation_forest:
            logger.info("ðŸš¨ Anomaly detection monitoring disabled (no isolation forest)")
            return
        
        while self.is_running:
            try:
                # Collect recent metrics for anomaly detection
                recent_metrics = self._collect_recent_metrics()
                
                if len(recent_metrics) >= 10:  # Need sufficient data
                    # Detect anomalies using isolation forest
                    anomaly_scores = self.isolation_forest.decision_function(recent_metrics)
                    
                    # Check for anomalies
                    for i, score in enumerate(anomaly_scores[-5:]):  # Check last 5 points
                        if score < -self.anomaly_threshold:
                            await self._handle_anomaly_detection(score, recent_metrics[i])
                
                await asyncio.sleep(30)  # Check every 30 seconds
                
            except Exception as e:
                logger.error(f"Error in anomaly detection: {e}")
                await asyncio.sleep(60)
    
    async def _profile_performance(self):
        """Profile application performance"""
        
        if not self.enable_profiling:
            return
        
        # Start memory tracking
        tracemalloc.start()
        
        while self.is_running:
            try:
                # Memory profiling
                current, peak = tracemalloc.get_traced_memory()
                self.metrics_history['memory_current'].append(current / 1024 / 1024)  # MB
                self.metrics_history['memory_peak'].append(peak / 1024 / 1024)  # MB
                
                await asyncio.sleep(30)
                
            except Exception as e:
                logger.error(f"Error in performance profiling: {e}")
                await asyncio.sleep(60)
    
    async def _check_system_alerts(self, cpu_percent: float, memory_percent: float, disk_percent: float):
        """Check system metrics for alert conditions"""
        
        alerts = []
        
        # CPU alerts
        if cpu_percent > 90:
            alerts.append(MonitoringAlert(
                id=f"cpu_high_{int(time.time())}",
                severity=AlertSeverity.ERROR,
                message=f"High CPU usage: {cpu_percent:.1f}%",
                timestamp=datetime.now(),
                metric_name="cpu_usage",
                value=cpu_percent,
                threshold=90.0
            ))
        
        # Memory alerts
        if memory_percent > 95:
            alerts.append(MonitoringAlert(
                id=f"memory_high_{int(time.time())}",
                severity=AlertSeverity.CRITICAL,
                message=f"Critical memory usage: {memory_percent:.1f}%",
                timestamp=datetime.now(),
                metric_name="memory_usage",
                value=memory_percent,
                threshold=95.0
            ))
        
        # Disk alerts
        if disk_percent > 90:
            alerts.append(MonitoringAlert(
                id=f"disk_high_{int(time.time())}",
                severity=AlertSeverity.WARNING,
                message=f"High disk usage: {disk_percent:.1f}%",
                timestamp=datetime.now(),
                metric_name="disk_usage",
                value=disk_percent,
                threshold=90.0
            ))
        
        # Send alerts
        for alert in alerts:
            await self._send_alert(alert)
    
    async def _send_alert(self, alert: MonitoringAlert):
        """Send alert through configured channels"""
        
        self.alerts.append(alert)
        
        alert_message = f"ðŸš¨ {alert.severity.value.upper()}: {alert.message}"
        
        # Slack alerts
        if 'slack' in self.alert_channels:
            try:
                await self._send_slack_alert(alert_message, alert)
            except Exception as e:
                logger.error(f"Failed to send Slack alert: {e}")
        
        # Email alerts
        if 'email' in self.alert_channels:
            try:
                await self._send_email_alert(alert_message, alert)
            except Exception as e:
                logger.error(f"Failed to send email alert: {e}")
        
        # Discord alerts
        if 'discord' in self.alert_channels:
            try:
                await self._send_discord_alert(alert_message, alert)
            except Exception as e:
                logger.error(f"Failed to send Discord alert: {e}")
        
        logger.warning(f"Alert sent: {alert_message}")
    
    async def _send_slack_alert(self, message: str, alert: MonitoringAlert):
        """Send alert to Slack"""
        
        if not hasattr(self, 'slack_client'):
            return
        
        channel = os.getenv('SLACK_ALERT_CHANNEL', '#kimera-alerts')
        
        self.slack_client.chat_postMessage(
            channel=channel,
            text=message,
            attachments=[{
                "color": "danger" if alert.severity in [AlertSeverity.ERROR, AlertSeverity.CRITICAL] else "warning",
                "fields": [
                    {"title": "Metric", "value": alert.metric_name, "short": True},
                    {"title": "Value", "value": f"{alert.value:.2f}", "short": True},
                    {"title": "Threshold", "value": f"{alert.threshold:.2f}", "short": True},
                    {"title": "Timestamp", "value": alert.timestamp.isoformat(), "short": True}
                ]
            }]
        )
    
    async def _send_email_alert(self, message: str, alert: MonitoringAlert):
        """Send alert via email"""
        
        # Implementation would use SMTP library
        pass
    
    async def _send_discord_alert(self, message: str, alert: MonitoringAlert):
        """Send alert to Discord webhook"""
        
        # Implementation would use Discord webhook
        pass
    
    def _collect_recent_metrics(self) -> List[List[float]]:
        """Collect recent metrics for anomaly detection"""
        
        metrics = []
        metric_names = ['cpu_usage', 'memory_usage', 'error_rate']
        
        min_length = min(len(self.metrics_history[name]) for name in metric_names if self.metrics_history[name])
        
        if min_length == 0:
            return []
        
        for i in range(min_length):
            metrics.append([
                self.metrics_history[name][i] for name in metric_names
            ])
        
        return metrics
    
    def _calculate_error_rate(self) -> float:
        """Calculate current error rate"""
        
        # This would integrate with actual error tracking
        return 0.0
    
    async def _handle_anomaly_detection(self, score: float, metrics: List[float]):
        """Handle detected anomaly"""
        
        alert = MonitoringAlert(
            id=f"anomaly_{int(time.time())}",
            severity=AlertSeverity.WARNING,
            message=f"Anomaly detected with score {score:.3f}",
            timestamp=datetime.now(),
            metric_name="anomaly_score",
            value=score,
            threshold=self.anomaly_threshold,
            context={"metrics": metrics}
        )
        
        await self._send_alert(alert)
    
    def get_monitoring_status(self) -> Dict[str, Any]:
        """Get current monitoring status"""
        
        return {
            "monitoring_level": self.monitoring_level.value,
            "is_running": self.is_running,
            "start_time": self.start_time.isoformat(),
            "uptime_seconds": (datetime.now() - self.start_time).total_seconds(),
            "background_tasks": len(self.background_tasks),
            "alerts_count": len(self.alerts),
            "recent_alerts": [
                {
                    "id": alert.id,
                    "severity": alert.severity.value,
                    "message": alert.message,
                    "timestamp": alert.timestamp.isoformat()
                }
                for alert in list(self.alerts)[-10:]  # Last 10 alerts
            ],
            "capabilities": {
                "tracing": self.enable_tracing,
                "profiling": self.enable_profiling,
                "anomaly_detection": self.enable_anomaly_detection,
                "nvidia_monitoring": NVIDIA_MONITORING,
                "mlops_monitoring": MLOPS_MONITORING,
                "alert_channels": self.alert_channels
            }
        }
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """Get summary of current metrics"""
        
        summary = {}
        
        for metric_name, history in self.metrics_history.items():
            if history:
                summary[metric_name] = {
                    "current": history[-1],
                    "average": np.mean(history),
                    "min": np.min(history),
                    "max": np.max(history),
                    "count": len(history)
                }
        
        return summary


# Global monitoring instance
_monitoring_core: Optional[KimeraMonitoringCore] = None


def get_monitoring_core() -> KimeraMonitoringCore:
    """Get the global monitoring core instance"""
    global _monitoring_core
    
    if _monitoring_core is None:
        try:
            _monitoring_core = KimeraMonitoringCore()
        except Exception as e:
            logger.error(f"Failed to create monitoring core: {e}")
            # If creation fails, try to return a minimal instance
            _monitoring_core = None
            raise
    
    return _monitoring_core


def initialize_monitoring(
    monitoring_level: MonitoringLevel = MonitoringLevel.DETAILED,
    enable_tracing: bool = True,
    enable_profiling: bool = True,
    enable_anomaly_detection: bool = True
) -> KimeraMonitoringCore:
    """Initialize the Kimera monitoring system"""
    
    global _monitoring_core
    
    _monitoring_core = KimeraMonitoringCore(
        monitoring_level=monitoring_level,
        enable_tracing=enable_tracing,
        enable_profiling=enable_profiling,
        enable_anomaly_detection=enable_anomaly_detection
    )
    
    return _monitoring_core


async def start_monitoring():
    """Start the monitoring system"""
    monitoring = get_monitoring_core()
    await monitoring.start_monitoring()


async def stop_monitoring():
    """Stop the monitoring system"""
    if _monitoring_core:
        await _monitoring_core.stop_monitoring()


# Export main classes and functions
__all__ = [
    'KimeraMonitoringCore',
    'MonitoringLevel',
    'AlertSeverity',
    'KimeraMetric',
    'MonitoringAlert',
    'get_monitoring_core',
    'initialize_monitoring',
    'start_monitoring',
    'stop_monitoring'
] 