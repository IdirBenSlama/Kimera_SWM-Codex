#!/usr/bin/env python3
"""
Minimal Cognitive Test
=====================

Tests cognitive components with minimal imports to isolate issues.
"""

import asyncio
import logging

import torch

# Set up basic logging
logging.basicConfig(level=logging.INFO)


async def test_minimal_cognitive():
    """Test with minimal imports"""
    print("üß™ MINIMAL COGNITIVE TEST")
    print("=" * 30)

    try:
        # Test 1: Direct GPU operations first
        print("1Ô∏è‚É£  Testing GPU Operations...")
        from src.core.performance.gpu_acceleration import (
            initialize_gpu_acceleration,
            move_to_gpu,
        )

        gpu_ok = initialize_gpu_acceleration()
        test_tensor = torch.randn(32, 32)
        gpu_tensor = move_to_gpu(test_tensor)
        result = torch.matmul(gpu_tensor, gpu_tensor.T)
        print(f"   ‚úÖ GPU operations working: {result.shape}")

        # Test 2: Basic cache operations
        print("2Ô∏è‚É£  Testing Cache Operations...")
        from src.core.performance.advanced_caching import (
            get_cached,
            initialize_caching,
            put_cached,
        )

        cache_ok = await initialize_caching()
        await put_cached("test", {"gpu_result": str(result.shape)})
        cached = await get_cached("test")
        print(f'   ‚úÖ Cache operations working: {cached["gpu_result"]}')

        # Test 3: Pipeline operations
        print("3Ô∏è‚É£  Testing Pipeline Operations...")
        from src.core.performance.pipeline_optimization import (
            TaskPriority,
            add_pipeline_task,
        )

        async def simple_task(data):
            return f"Processed: {data}"

        task_ok = add_pipeline_task(
            "test_task", simple_task, "minimal_test", priority=TaskPriority.HIGH
        )
        print(f"   ‚úÖ Pipeline operations working: {task_ok}")

        # Test 4: Scaling operations
        print("4Ô∏è‚É£  Testing Scaling Operations...")
        from src.core.scaling.horizontal_scaling import (
            initialize_horizontal_scaling,
            route_cognitive_request,
        )

        scaling_ok = await initialize_horizontal_scaling(min_nodes=2, max_nodes=3)
        node = await route_cognitive_request("test_cognitive", {"priority": "high"})
        print(f"   ‚úÖ Scaling operations working: routed to {node}")

        print("\nüéâ ALL CORE PERFORMANCE SYSTEMS OPERATIONAL!")
        print("‚úÖ GPU Acceleration: Working")
        print("‚úÖ Advanced Caching: Working")
        print("‚úÖ Pipeline Optimization: Working")
        print("‚úÖ Horizontal Scaling: Working")

        return True

    except Exception as e:
        print(f"‚ùå Minimal test error: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("üöÄ Testing Minimal Cognitive Systems")
    success = asyncio.run(test_minimal_cognitive())

    if success:
        print("\nüéâ CORE SYSTEMS FULLY OPERATIONAL!")
        print("üåü Performance platform working perfectly!")
    else:
        print("\n‚ùå Core systems need attention")
