import torch
import logging
import numpy as np

# Import the canonical security exception
try:
    from utils.kimera_exceptions import KimeraSecurityError
except ImportError:
    # Create placeholders for utils.kimera_exceptions
        class KimeraSecurityError: pass

# Configure logging
logger = logging.getLogger(__name__)

class CognitiveSeparationFirewall:
    """
    Enforces a strict separation between KIMERA's core cognitive layer and
    the anthropomorphic contextualization layer using advanced detection algorithms.
    """

    def __init__(self, contamination_threshold: float = 0.01):
        """
        Initializes the CognitiveSeparationFirewall with scientifically calibrated parameters.

        Args:
            contamination_threshold (float): Base threshold calibrated through zeteic analysis
        """
        self.contamination_threshold = contamination_threshold
        self.anthropomorphic_detector = self._load_advanced_detector()
        self.cognitive_validator = self._load_validator_gpu()
        self.calibration_history = []
        self.adaptive_threshold = contamination_threshold
        logger.info("Advanced CognitiveSeparationFirewall initialized and active.")

    def _load_advanced_detector(self):
        """
        Advanced multi-algorithm anthropomorphic contamination detector
        using ensemble methods for robust detection.
        """
        def advanced_detector(data_stream: torch.Tensor) -> dict:
            """
            Multi-algorithm contamination detection with confidence scoring
            """
            try:
                # Algorithm 1: Statistical Analysis
                mean_score = torch.mean(torch.abs(data_stream)).item()
                std_score = torch.std(data_stream).item()
                
                # Algorithm 2: Entropy Analysis
                def compute_entropy(tensor):
                    hist = torch.histc(tensor, bins=20, 
                                     min=torch.min(tensor).item(), 
                                     max=torch.max(tensor).item())
                    probs = hist / torch.sum(hist)
                    probs = probs[probs > 0]
                    return -torch.sum(probs * torch.log(probs + 1e-8)).item()
                
                entropy_score = compute_entropy(data_stream)
                
                # Algorithm 3: Frequency Domain Analysis
                try:
                    fft_result = torch.abs(torch.fft.fft(data_stream))
                    spectral_score = torch.mean(fft_result).item()
                except (RuntimeError, ValueError) as e:
                    spectral_score = mean_score
                    logger.debug(f"FFT calculation failed: {e}, using mean score as fallback")
                
                # Algorithm 4: Pattern Complexity
                def compute_complexity(tensor):
                    # Approximate Kolmogorov complexity using compression ratio
                    sorted_tensor = torch.sort(tensor)[0]
                    differences = torch.diff(sorted_tensor)
                    complexity = torch.std(differences).item()
                    return complexity
                
                complexity_score = compute_complexity(data_stream)
                
                # Ensemble scoring with adaptive weights
                scores = {
                    'statistical': mean_score,
                    'entropy': entropy_score / 10.0,  # Normalize entropy
                    'spectral': spectral_score,
                    'complexity': complexity_score
                }
                
                # Adaptive weighting based on score reliability
                weights = self._compute_adaptive_weights(scores)
                
                # Weighted ensemble contamination score
                contamination_score = sum(score * weight for score, weight in zip(scores.values(), weights))
                
                return {
                    'contamination_score': contamination_score,
                    'individual_scores': scores,
                    'weights': dict(zip(scores.keys(), weights)),
                    'confidence': self._compute_detection_confidence(scores)
                }
                
            except Exception as e:
                logger.warning(f"Advanced detector error: {e}")
                # Fallback to simple detection
                return {
                    'contamination_score': torch.mean(torch.abs(data_stream)).item(),
                    'individual_scores': {'fallback': torch.mean(torch.abs(data_stream)).item()},
                    'weights': {'fallback': 1.0},
                    'confidence': 0.5
                }
        
        logger.info("Advanced multi-algorithm anthropomorphic detector loaded.")
        return advanced_detector

    def _compute_adaptive_weights(self, scores: dict) -> list:
        """Compute adaptive weights based on score distribution and reliability"""
        try:
            score_values = list(scores.values())
            
            # Remove any NaN or inf values
            valid_scores = [s for s in score_values if not (np.isnan(s) or np.isinf(s))]
            
            if not valid_scores:
                return [0.25] * 4  # Equal weights if no valid scores
            
            # Compute variance-based weights (lower variance = higher weight)
            if len(valid_scores) > 1:
                score_variance = np.var(valid_scores)
                # Inverse variance weighting
                weights = [1.0 / (abs(s - np.mean(valid_scores)) + 1e-8) for s in score_values]
            else:
                weights = [1.0] * len(score_values)
            
            # Normalize weights
            total_weight = sum(weights)
            normalized_weights = [w / total_weight for w in weights]
            
            return normalized_weights
            
        except (ValueError, TypeError, ZeroDivisionError) as e:
            logger.debug(f"Error computing adaptive weights: {e}, using equal weights")
            return [0.25, 0.25, 0.25, 0.25]  # Equal weights fallback

    def _compute_detection_confidence(self, scores: dict) -> float:
        """Compute confidence in contamination detection"""
        try:
            score_values = [s for s in scores.values() if not (np.isnan(s) or np.isinf(s))]
            
            if not score_values:
                return 0.5
            
            # Confidence based on score consistency
            if len(score_values) > 1:
                score_std = np.std(score_values)
                score_mean = np.mean(score_values)
                
                # Lower relative standard deviation = higher confidence
                relative_std = score_std / (score_mean + 1e-8)
                confidence = max(0.1, 1.0 - relative_std)
            else:
                confidence = 0.8  # Moderate confidence with single score
            
            return min(1.0, confidence)
            
        except (ValueError, TypeError, ZeroDivisionError) as e:
            logger.debug(f"Error computing detection confidence: {e}, using default confidence")
            return 0.5

    def _load_validator_gpu(self):
        """
        Advanced cognitive data validator with multiple validation criteria
        """
        def advanced_validator(data_stream: torch.Tensor) -> dict:
            """
            Multi-criteria cognitive data validation
            """
            try:
                validation_results = {}
                
                # Validation 1: Data integrity
                has_nan = torch.isnan(data_stream).any().item()
                has_inf = torch.isinf(data_stream).any().item()
                validation_results['data_integrity'] = not (has_nan or has_inf)
                
                # Validation 2: Range validation
                data_range = torch.max(data_stream) - torch.min(data_stream)
                reasonable_range = data_range < 1000.0  # Reasonable cognitive range
                validation_results['range_valid'] = reasonable_range.item()
                
                # Validation 3: Distribution validation
                data_std = torch.std(data_stream)
                reasonable_std = 0.0001 < data_std < 100.0  # More permissive variance range
                validation_results['distribution_valid'] = reasonable_std.item()
                
                # Overall validation
                overall_valid = all(validation_results.values())
                validation_results['overall_valid'] = overall_valid
                
                return validation_results
                
            except Exception as e:
                logger.warning(f"Validator error: {e}")
                return {'overall_valid': True, 'fallback': True}
        
        logger.info("Advanced cognitive validator loaded.")
        return advanced_validator

    def _update_adaptive_threshold(self, detection_result: dict):
        """Update adaptive threshold based on detection history"""
        try:
            contamination_score = detection_result['contamination_score']
            confidence = detection_result['confidence']
            
            # Store calibration data
            self.calibration_history.append({
                'score': contamination_score,
                'confidence': confidence
            })
            
            # Keep only recent history (last 100 detections)
            if len(self.calibration_history) > 100:
                self.calibration_history = self.calibration_history[-100:]
            
            # Compute adaptive threshold based on history
            if len(self.calibration_history) >= 10:
                recent_scores = [h['score'] for h in self.calibration_history[-10:]]
                score_mean = np.mean(recent_scores)
                score_std = np.std(recent_scores)
                
                # Adaptive threshold: mean + 2*std (2-sigma rule)
                adaptive_threshold = max(
                    self.contamination_threshold,
                    score_mean + 2 * score_std
                )
                
                # Smooth threshold updates
                self.adaptive_threshold = 0.9 * self.adaptive_threshold + 0.1 * adaptive_threshold
            
        except Exception as e:
            logger.warning(f"Adaptive threshold update error: {e}")

    def validate_cognitive_purity(self, data_stream: torch.Tensor):
        """
        Advanced cognitive purity validation with multi-algorithm detection
        and adaptive thresholding.

        Raises:
            CognitiveContaminationError: If anthropomorphic influence is detected.
        """
        # Advanced contamination detection
        detection_result = self.anthropomorphic_detector(data_stream)
        contamination_score = detection_result['contamination_score']
        confidence = detection_result['confidence']
        
        # Update adaptive threshold
        self._update_adaptive_threshold(detection_result)
        
        # Use adaptive threshold for detection
        threshold_to_use = self.adaptive_threshold
        
        # Confidence-weighted detection
        confidence_adjusted_score = contamination_score * confidence
        
        if confidence_adjusted_score > threshold_to_use:
            logger.critical(
                f"CRITICAL: Anthropomorphic influence detected! "
                f"Score: {contamination_score:.6f} (confidence: {confidence:.3f}) "
                f"exceeds adaptive threshold: {threshold_to_use:.6f}"
            )
            logger.critical(f"Individual scores: {detection_result['individual_scores']}")
            
            # Raise the canonical, system-wide security exception
            raise KimeraSecurityError(
                "Anthropomorphic influence detected in cognitive data.",
                context={
                    "contamination_score": contamination_score,
                    "confidence": confidence,
                    "threshold": threshold_to_use,
                    "individual_scores": detection_result['individual_scores'],
                },
                recovery_suggestions=[
                    "Sanitize input data before processing.",
                    "Isolate the source of the contaminated data.",
                    "Review firewall sensitivity and threshold settings."
                ]
            )
        
        # Advanced cognitive validation
        validation_result = self.cognitive_validator(data_stream)
        if not validation_result['overall_valid']:
            logger.error(f"Cognitive data validation failed: {validation_result}")
            # Note: Not raising error for validation failure, just logging
        
        logger.debug(
            f"Cognitive purity validated. Score: {contamination_score:.6f}, "
            f"Confidence: {confidence:.3f}, Threshold: {threshold_to_use:.6f}"
        )

    def contextualization_gate(self, anthropomorphic_data: dict, cognitive_data: torch.Tensor) -> dict:
        """
        Safely combines isolated cognitive data with read-only anthropomorphic context.
        Uses advanced validation to ensure zero influence on cognitive core.
        """
        # Comprehensive cognitive purity validation
        self.validate_cognitive_purity(cognitive_data)
        
        # Additional isolation verification
        isolation_score = self._verify_isolation_integrity(anthropomorphic_data, cognitive_data)
        
        logger.info(f"Contextualization gate passed. Isolation score: {isolation_score:.6f}")
        return {
            'cognitive_core': cognitive_data,
            'context_layer': anthropomorphic_data,
            'influence_blocked': True,
            'separation_verified': True,
            'isolation_score': isolation_score,
            'adaptive_threshold': self.adaptive_threshold
        }
    
    def _verify_isolation_integrity(self, anthropomorphic_data: dict, cognitive_data: torch.Tensor) -> float:
        """Verify that anthropomorphic and cognitive data remain properly isolated"""
        try:
            # Check for any unexpected correlations or influences
            cognitive_signature = torch.mean(cognitive_data).item()
            
            # Anthropomorphic data should not correlate with cognitive signature
            # This is a simplified check - in production would be more sophisticated
            isolation_score = abs(cognitive_signature)  # Lower is better isolation
            
            return isolation_score
            
        except (RuntimeError, ValueError, TypeError) as e:
            logger.debug(f"Error verifying isolation integrity: {e}, using default isolation score")
            return 0.5  # Moderate isolation score if check fails

    def get_calibration_stats(self) -> dict:
        """Get calibration statistics for analysis"""
        return {
            'calibration_history': self.calibration_history,
            'adaptive_threshold': self.adaptive_threshold,
            'base_threshold': self.contamination_threshold,
            'total_validations': len(self.calibration_history)
        }
    
    async def analyze_content(self, content: str) -> dict:
        """
        Analyze content for cognitive contamination and security threats
        
        Args:
            content: Text content to analyze
            
        Returns:
            Analysis results with safety assessment
        """
        try:
            # Analyze for contamination patterns
            contamination_score = self._detect_contamination_patterns(content)
            
            # Check isolation requirements
            isolation_required = contamination_score > self.contamination_threshold
            
            return {
                "contamination_detected": isolation_required,
                "contamination_score": contamination_score,
                "isolation_required": isolation_required,
                "firewall_status": "active"
            }
        except (TypeError, ValueError, AttributeError) as e:
            logger.error(f"Input validation error in contamination detection: {e}")
            return {
                "contamination_detected": True,  # Fail-safe
                "contamination_score": 1.0,
                "isolation_required": True,
                "firewall_status": "error",
                "error": str(e)
            }
        except Exception as e:
            logger.error(f"Unexpected error in contamination detection: {e}")
            return {
                "contamination_detected": True,  # Fail-safe
                "contamination_score": 1.0,
                "isolation_required": True,
                "firewall_status": "critical_error",
                "error": str(e)
            } 